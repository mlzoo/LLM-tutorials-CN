{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8614effe",
   "metadata": {},
   "source": [
    "\n",
    "# 基于 QLoRA 的 LLama3-ChatQA-8B fine-tuning \n",
    "\n",
    "在深度学习领域，大型预训练语言模型（如LLaMA）已经显示出在各种自然语言处理任务上的卓越性能。然而，这些模型的庞大规模往往伴随着巨大的存储和计算需求。为了解决这一问题，本文将介绍如何使用QLoRA技术和4-bit量化技术来微调大型语言模型，以实现性能和效率的平衡。\n",
    "\n",
    "## 前置知识：名词解释\n",
    "\n",
    "- QLoRA（Quantized LoRA）：QLoRA是一种模型微调技术，它通过在模型的注意力机制中引入可学习的低秩矩阵来增强模型的表示能力。这种方法不仅能够显著提升模型的性能，而且由于其低秩特性，还能有效控制参数的增加，从而减少模型的存储和计算需求。\n",
    "\n",
    "![](../../images/01-lora-image.png)\n",
    "\n",
    "\n",
    "- NF4量化：NF4的全称为：Normalized 4-bit Floating Point Quantization，旨在减少模型的存储和计算需求，同时保持模型性能。在LLM过程训练中，NF4量化可以提高模型的效率，尤其是在资源受限的环境中。\n",
    "- bitsandbytes： Python 库，提供了对 CUDA 自定义函数的轻量级封装，尤其是针对 8-bit优化器、矩阵乘法和量化函数。\n",
    "\n",
    "\n",
    "## 代码\n",
    "\n",
    "### 1 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b498322f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-28 15:46:44,843] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,  # 用于加载预训练的语言模型\n",
    "    AutoTokenizer,  # 用于加载与模型相匹配的分词器\n",
    "    BitsAndBytesConfig,  # 用于配置4-bit量化\n",
    "    HfArgumentParser,  # 用于解析命令行参数\n",
    "    TrainingArguments,  # 用于设置训练参数\n",
    "    pipeline,  # 用于创建模型的pipeline\n",
    "    logging,  # 用于记录日志\n",
    ")\n",
    "from peft import LoraConfig, PeftModel  # 用于配置和加载QLoRA模型\n",
    "from trl import SFTTrainer  # 用于执行监督式微调的Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15265d8",
   "metadata": {},
   "source": [
    "### 2 设置模型和数据集\n",
    "\n",
    "设置预训练模型的名称、数据集名称以及微调后的模型名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c3a03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置预训练模型的名称\n",
    "model_name = \"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "\n",
    "# 设置要使用的指令数据集名称\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# 设置微调后模型的名称\n",
    "new_model = \"llama-2-7b-fine-tuned\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf09a9",
   "metadata": {},
   "source": [
    "### 3 配置参数\n",
    "\n",
    "设置QLoRA和4-bit量化的关键参数，包括LoRA的注意力维度、缩放因子、dropout概率，以及4-bit量化的精度和量化类型。\n",
    "\n",
    "#### 3.1 QLoRA参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e26c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 64  # LoRA的注意力维度\n",
    "\n",
    "# Alpha参数用于LoRA缩放\n",
    "lora_alpha = 16\n",
    "\n",
    "# LoRA层的dropout概率\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ef139e",
   "metadata": {},
   "source": [
    "#### 3.2 bitsandbytes参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c3f87fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活4-bit精度的基础模型加载\n",
    "use_4bit = True\n",
    "\n",
    "# 4-bit基础模型的计算数据类型\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# 4-bit量化类型（fp4或nf4）\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# 激活4-bit基础模型的嵌套量化（双重量化）\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718f592",
   "metadata": {},
   "source": [
    "#### 3.3 TrainingArguments参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e710174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出目录，用于存储模型预测和检查点\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# 训练周期数\n",
    "num_train_epochs = 1\n",
    "\n",
    "# 是否启用fp16/bf16训练（在A100上将bf16设置为True）\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# GPU上每个训练批次的样本数\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# GPU上每个评估批次的样本数\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# 累积梯度的更新步骤数\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# 是否启用梯度检查点\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# 最大梯度归一化（梯度裁剪）\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# 初始学习率（AdamW优化器）\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# 权重衰减，应用于全部layer（不包括bias/LayerNorm的权重）\n",
    "weight_decay = 0.001\n",
    "\n",
    "# 优化器\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# 学习率计划\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# 训练步数（覆盖num_train_epochs）\n",
    "max_steps = -1\n",
    "\n",
    "# 线性预热的步数比率（从0到学习率）\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# 按长度分组序列\n",
    "group_by_length = True\n",
    "\n",
    "# 每X更新步骤保存检查点\n",
    "save_steps = 0\n",
    "\n",
    "# 每X更新步骤记录日志\n",
    "logging_steps = 25\n",
    "\n",
    "# SFT参数配置\n",
    "# 最大序列长度\n",
    "max_seq_length = None\n",
    "\n",
    "# 打包多个短示例到同一输入序列以提高效率\n",
    "packing = False\n",
    "\n",
    "# 将整个模型加载到GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5901d",
   "metadata": {},
   "source": [
    "### 4 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f772a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df060c9e",
   "metadata": {},
   "source": [
    "### 5 配置量化bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2d1cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07180ad1",
   "metadata": {},
   "source": [
    "### 6 检查GPU与bfloat16的兼容性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ed4986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU支持bfloat16\n"
     ]
    }
   ],
   "source": [
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"GPU支持bfloat16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f0ba98",
   "metadata": {},
   "source": [
    "### 7 加载模型\n",
    "\n",
    "#### 7.1 加载基础模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df15ec60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5d6796dfb949a095e06bccae1cbb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff42d3d",
   "metadata": {},
   "source": [
    "#### 7.2 加载tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39f69c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # 修复fp16训练中的溢出问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401a2f5",
   "metadata": {},
   "source": [
    "### 8 加载LoRA配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5cc5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a37d3b",
   "metadata": {},
   "source": [
    "### 9 设置TrainingArguments训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2670fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e864b",
   "metadata": {},
   "source": [
    "### 10 训练模型\n",
    "\n",
    "使用`SFTTrainer`进行模型的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc1d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 19/250 00:57 < 13:01, 0.30 it/s, Epoch 0.07/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 设置监督式微调参数\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e5dc4",
   "metadata": {},
   "source": [
    "### 11 保存微调后的模型\n",
    "\n",
    "训练完成后，保存微调后的模型以便后续使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad688ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23739e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
